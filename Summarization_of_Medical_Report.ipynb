{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-c04oZj36H8"
   },
   "source": [
    "* This notebook performs data cleaning, chunked summarization, and evaluation\n",
    "* On the MTSamples dataset using BART-based transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "El5PunI-4BSO",
    "outputId": "3a29b3e7-149f-490b-f65b-8eb7c14e1fe4"
   },
   "outputs": [],
   "source": [
    "# Install and Import Dependencies\n",
    "!pip install -q pandas transformers nltk datasets evaluate rouge_score bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DitfOG3g4SRA",
    "outputId": "31f836b2-ba04-45cc-d4a0-a747dc9ba921"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDPj10Au4R4l",
    "outputId": "95aa5416-1cb5-4488-afb8-d259e35bf35a"
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(\"mtsamples.csv\")\n",
    "print(\"Original record count:\", len(df))\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5FwVdEQ5Can",
    "outputId": "503d4e80-41e7-44ac-a7bd-b85514965ae2"
   },
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df = df.dropna(subset=['transcription', 'description']).copy()\n",
    "    df['cleaned_transcription'] = df['transcription'].astype(str).apply(clean_text)\n",
    "    return df\n",
    "\n",
    "df_cleaned = preprocess_data(df)\n",
    "print(\"Cleaned record count:\", len(df_cleaned))\n",
    "print(df_cleaned[['description', 'cleaned_transcription']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350,
     "referenced_widgets": [
      "f514f9695c5744e1bdb3f03be6565c28",
      "cdb179564c1a4c7c86b35fba8f821c8a",
      "6e439dfd2a3c4a479ae1599689a37878",
      "ac25e4bf06c7491bbf2e0ec58ef195aa",
      "3f379ab3a00f475dabb417418db39a1a",
      "281b16d3a0124318b254e2b52c72b000",
      "355665f7210945a5a0657af304dddc9a",
      "4468a772080441eea560304e4bb08fef",
      "3cd7a7696a6a465cbe5b7bc2116929aa",
      "e3f0b3d7690a490f92cc780cbd306ca1",
      "42718294f08a4dc3916398d91ed37739",
      "7efbe73f498f4f91a01a31180a29bf60",
      "f6ebca3cf88a458f9a6330f183f98224",
      "9f90b8b1d47c46c69adbae7465a9c79f",
      "18ab0ee6d7ec4e02805f962882af7115",
      "c04fb261b34742b8be7fbb52e5f636d0",
      "c164c11f894d444ba16dbe1e182526a4",
      "cd9a79f067e04d4fa8699b1bd458ee38",
      "4389547cdf0943779b8b146bd489bb63",
      "4fdb5a0eadf9494c8c4e91a53b010110",
      "ae9c9d1cf9c14f2f97b4c9c4455a9974",
      "de68dfc976264f50bdc6f0c158e57a34",
      "147ad91a11724c6d8cbaf044e7a4a91e",
      "8ed25d5debb7467e8ae170ca9cfbbfef",
      "e3aef4e6c1a044098359aa9542e54763",
      "124e17a9576f42aa93ac60eb2102a84e",
      "44ab19faea604cecab8f34641ab208ff",
      "533fda171ef445228f33d9ed200ddf35",
      "bc19825f804e41eda39841ff3341b8e6",
      "55d8dc6b55fe43e69f9148b30e9229b0",
      "94fe8f739c8446448135beb437efcb27",
      "8a306f91f8fa4c46bc90db33f4ced7d2",
      "00eb9c0f489746fdbb94ca5e24d100e6",
      "5ed2c6a321dd43bc8bc6000fd59b02e4",
      "f0c356db263e4656a8ef0fe51563dda9",
      "02e506e3dbfd4d9d9dc56f1f9e7ed165",
      "c6eb21277f74402f80d337a5c0cf5f77",
      "820970da9fba411c9a18dbd28d47565e",
      "c088cac3625440dca3667af4c6615f62",
      "90dbda754cab4f8da04ecc66e2a291e3",
      "6c18a9206fba4076a2782eeac772d2b2",
      "3f778965e6384c9282ad3011257ab441",
      "0c5fed1589534102bd4a3305375de6f1",
      "9d39a0f3f1aa4f5eab5a0e00ed7e4196",
      "ef2284d702e14d088ebc40829a364f93",
      "6fe613a14198410da7da33f544fa01c5",
      "b73ede33179d43cbad1826176fe0cdc5",
      "c6c756ba9a5a41199a1eb3ce82b52162",
      "8b8bcc214eba43b9a9ac80a4a3a50278",
      "0ac57599da4b4adb83231436484fc5ec",
      "9bee9ac44b984abaaa440272e075952e",
      "0da3cb076050479194f7c4c827d17bec",
      "dd5b814a33254c81902ca62dd492e158",
      "7829fb6ff57d4e59a36c93b5ecf1f083",
      "a22e49ad43c1485cacb05b66dc7e7436",
      "d376bbb638d74abc98a8307e82648ac6",
      "686d9046993e4dd7996134ea4009ecf0",
      "08cc28a5d9474673afc5f7cc9f65f959",
      "d71340523a0c419ab3be5bd6f6557980",
      "c0741baabee1402d9a43af6425f52b4d",
      "771ccad7489946008b420ed1d9d43d3d",
      "8b9ceae7c68b45a4868e7a1470173823",
      "db1f5303b0ef47d3b3101c3dd4789498",
      "479d324279ba466b8aae30c2bd7ffab9",
      "eb7cb3f6e51d469a98129a0612510a0d",
      "171fc345802544e492a92b81e1428964"
     ]
    },
    "id": "RsjT27y_-SB4",
    "outputId": "b5a2924f-c6ab-4959-d459-db056b130d87"
   },
   "outputs": [],
   "source": [
    "# Load Summarization Model\n",
    "MODEL_NAME = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCEqYK9n_SZs"
   },
   "outputs": [],
   "source": [
    "# Text Chuncking\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "def chunk_text(text, tokenizer, max_length=MAX_INPUT_LENGTH, overlap=CHUNK_OVERLAP):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        chunk = tokenizer.decode(tokens[start:end], skip_special_tokens=True)\n",
    "        chunks.append(chunk)\n",
    "        start += max_length - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsVi5s3YLZ2F"
   },
   "outputs": [],
   "source": [
    "# Summarization Function\n",
    "SUMMARY_MAX = 150\n",
    "SUMMARY_MIN = 40\n",
    "\n",
    "def generate_medical_summary(text):\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    input_len = len(tokenizer.encode(text))\n",
    "    if input_len < MAX_INPUT_LENGTH:\n",
    "        try:\n",
    "            max_len = min(SUMMARY_MAX, input_len)\n",
    "            return summarizer(text, max_length=max_len, min_length=SUMMARY_MIN, do_sample=False)[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            print(\"Single-pass error:\", e)\n",
    "            return \"\"\n",
    "    summaries = []\n",
    "    for chunk in chunk_text(text, tokenizer):\n",
    "        try:\n",
    "            input_len = len(tokenizer.encode(chunk))\n",
    "            max_len = min(SUMMARY_MAX, input_len)\n",
    "            summary = summarizer(chunk, max_length=max_len, min_length=SUMMARY_MIN, do_sample=False)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(\"Chunk error:\", e)\n",
    "            continue\n",
    "    return summarizer(\" \".join(summaries), max_length=SUMMARY_MAX*2, min_length=SUMMARY_MIN*2, do_sample=False)[0]['summary_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815,
     "referenced_widgets": [
      "e0e19700bf7c47a0b208311ef837d873",
      "80d02791c49246e5b46c9573a82ada49",
      "3b2e58ece2d0405cbe6ddbf198529d4a",
      "ffe618b35406406d99133026da9445c8",
      "575ffc2d389e4dd7b54cb17187035da0",
      "f84571216e4e4a449e611a7e41d3103c",
      "b2dd2e0ae32e4f98ab62fe3e026847ad",
      "8e6be08aa7fd415abb65b293391fc8b2",
      "767c42bfffad4d68b203a6a1cf15e45f",
      "a965af6e58d247c19c6c4ccff8b94df0",
      "7a2e52d204e9495fbd4ee29cb3581d05"
     ]
    },
    "id": "_dKaaYnX_t0v",
    "outputId": "3b2c0af0-54c1-4e3b-b0c8-7e34febb80d9"
   },
   "outputs": [],
   "source": [
    "# Batch Summarization of 30 Reports\n",
    "df_batch = df_cleaned.head(30).copy()\n",
    "summaries = []\n",
    "print(\"\\nGenerating summaries for 30 reports...\")\n",
    "for i, text in tqdm(enumerate(df_batch['cleaned_transcription']), total=30):\n",
    "    print(f\"Summarizing index {i}...\")\n",
    "    summaries.append(generate_medical_summary(text))\n",
    "df_batch['generated_summary'] = summaries\n",
    "df_batch.to_csv(\"batch_30_summaries.csv\", index=False)\n",
    "print(\"Saved to 'batch_30_summaries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "M_qbRrGqMoI0",
    "outputId": "6c714563-44be-439e-ad2b-6c9d3c4945e2"
   },
   "outputs": [],
   "source": [
    "# Display all 30 descriptions and their generated summaries\n",
    "df_batch[['description', 'generated_summary']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "b70b3ca1f22c4da7b07b182afba269d8",
      "1c27689b20fb4b10a4a0263b65bd89ad",
      "5cd1399a6ca4450fa05913275920ef4c",
      "1d3e3e37f69f4fce852e0b6f66adbb74",
      "2b1949ebfa254f26ad4e2f91dee3e43f",
      "2919380415a04f44a63382b527d33f1c",
      "1fee1f34201f4ffea730c419fb00cae7",
      "5fa01be9632e44d9b6163bff02726b4f",
      "66fdfb8fed474fbc8f8907c3718f63e7",
      "dd85a236bbfb4350b446db09074229b4",
      "60676b38310c4b6492ed54458561c03c",
      "00752acf160f4d4999c0372980bcbdd8",
      "8f9f2aeee4494a5e9cfa4be33cb7d0e8",
      "2bf8f64677b34a948a1cafe000b1ed78",
      "741ad35edb24427d9f12609f5b6849a8",
      "00989df032794599b6f26b7c8103820d",
      "8f87a138d2cd47058ddbd0784ab66121",
      "dc6a07e101ea4e928929828625f952e2",
      "a573f093347c4c359580d10d072f40b4",
      "881a73317c70437d9ed7c9eb0b93a21d",
      "d9140380f343405180c713978900bfea",
      "08c0961ecf57477eaa54e8798286297d",
      "f9bfa59ea4ad44268d3e6dcbe33e2528",
      "7ad22543bc274c23bcef29dc91793211",
      "6c98d46a24414710b4877387ba430464",
      "fcb0e16cd0f54406a8eae9a50c9cef79",
      "e08883deb0e34ad28c638f8e8d2758d3",
      "c192204e324d46e2b9b4cb8fbcf283aa",
      "a30d8589a2564ce880d6eaf3726efa8e",
      "c37e415b8e3041409d3d69475187504f",
      "715f657777b4401b94672337d67fa075",
      "dd868287a9ce4b6bb9ff1f5dbdf6fb33",
      "d47bd1987e87408596a89ebc807e6e87",
      "c51952c0d65a4bd7b72455006d262c55",
      "e25d988f54d8474182be479ac348b048",
      "27609d774adb474d8075ca84919f2e44",
      "cccdb775a3374fd18f7ca99c4dc3b2ad",
      "b1ee7c6c1b824f56956bf1cfe1ee1f46",
      "a9d1ca9e2ca347d1a53a66573542ef70",
      "18cd2f3b7b864e10946ef37ce5f418fc",
      "13b4bc65a8b846258782c993f2f9dd8a",
      "ef56f5c7e06040328f2d0e7d0d7b3fea",
      "f9bfa79646fe4e38a4144838efb6e2bd",
      "f328dd7c4f3d451da3041664b7b34ccc",
      "7075fe8164684619b346c19c0abbd3c1",
      "bd83321408ad40bdaa9750ec76493e8b",
      "da5b324c87334d7ead6f8ad759ee6991",
      "d92f4ef06f614b69b38143152921c3eb",
      "22fae751d2bd4e3b93fd3c5c6281d55a",
      "d10614fbc95446a49d45256555ea5b17",
      "69031710d616463e913cc73c68f7e7c5",
      "4304d3a1466a48bd982c6a014c616f41",
      "976d12ef198c46059a574c339bf13f2f",
      "c0308c9093f84e62b8c20b8d649943ce",
      "e5355b6c65ab4111b20c25ef01e8348f",
      "f7bc3fd41e8a4540ac022b04eb26aa62",
      "45f351d555c74f6e950d2b530219b59b",
      "6b42142ce86f47748275b7d309ba2c1f",
      "99807809c7db41d889351c7e54132a96",
      "4cea8379230d4d4bb22f069e8c1adbc3",
      "8e46495e952a4beaab91c28350387bdb",
      "23fc7fd1c5f645439f5b77c35a7f4275",
      "9f92256bb220455094e6b07822d9ca1b",
      "13e4c9f4f8364fafb8041ec291b7de5e",
      "f1a3714dc4c64e9f83ba87bc540a5456",
      "be268fac7698460c96fb79ecbd028d14",
      "aaa61096d8bd44df80e7ee66ebd739b3",
      "1e30d00edbb44e3183bf34e457448c54",
      "9c96d493ccf04590a9096292269752fd",
      "b3e31361b81c45308b6403bc54d06cc8",
      "dc82dc55e5eb4c5a8bea714a6039d75d",
      "f3142b3e1aed4a4faabba5222b67d177",
      "3a4113854eac43eb924e76a9b772f0e3",
      "4c677e465e1640b68e40c95cd352f53f",
      "1f8c307618604c439700d10fe9209a1e",
      "17a9e937da994786be68a9fec88004b2",
      "bdf5f8c367914e7e80bd8d04baafce48"
     ]
    },
    "id": "tuF1YrPUEpG2",
    "outputId": "53f3195e-59c1-49af-fabf-50982bebc27f"
   },
   "outputs": [],
   "source": [
    "# Evaluation (ROUGE + BERTScore)\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load('bertscore')\n",
    "\n",
    "refs = df_batch['description'].tolist()\n",
    "preds = df_batch['generated_summary'].tolist()\n",
    "valid = [(p, r) for p, r in zip(preds, refs) if p and r]\n",
    "\n",
    "if valid:\n",
    "    preds_valid, refs_valid = zip(*valid)\n",
    "    rouge_results = rouge.compute(predictions=preds_valid, references=refs_valid)\n",
    "    bert_results = bertscore.compute(predictions=preds_valid, references=refs_valid, model_type=\"bert-base-uncased\")\n",
    "    print(\"\\n ROUGE:\", rouge_results)\n",
    "    print(\"BERTScore F1 (mean):\", sum(bert_results['f1']) / len(bert_results['f1']))\n",
    "    pd.DataFrame([{\n",
    "        \"rouge1\": rouge_results['rouge1'],\n",
    "        \"rouge2\": rouge_results['rouge2'],\n",
    "        \"rougeL\": rouge_results['rougeL'],\n",
    "        \"bertscore_f1\": sum(bert_results['f1']) / len(bert_results['f1'])\n",
    "    }]).to_csv(\"evaluation_metrics.csv\", index=False)\n",
    "else:\n",
    "    print(\"No valid references/predictions to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "b3G4lqm7u9Er",
    "outputId": "ffe9a51a-c4d2-48dc-b1e1-90823c7f86a1"
   },
   "outputs": [],
   "source": [
    "# Evaluation Matrix Result\n",
    "\n",
    "# Evaluation results\n",
    "metrics = {\n",
    "    \"ROUGE-1\": 0.2328,\n",
    "    \"ROUGE-2\": 0.1759,\n",
    "    \"ROUGE-L\": 0.2176,\n",
    "    \"BERTScore F1\": 0.5597\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(metrics.keys(), metrics.values(), color=[\"#3498db\", \"#9b59b6\", \"#2ecc71\", \"#e67e22\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Evaluation Metrics for Medical Report Summarization\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Metric\")\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f\"{yval:.3f}\", ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPwiUpInCDve"
   },
   "outputs": [],
   "source": [
    "def generate_medical_summary(text, age=None, gender=None, allergies=None, diagnosis=None, medications=None):\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize and validate gender\n",
    "    if gender:\n",
    "        gender = gender.lower()\n",
    "        if gender not in [\"male\", \"female\", \"other\"]:\n",
    "            gender = \"unspecified\"\n",
    "\n",
    "    # Build demographic + clinical context\n",
    "    demographics = []\n",
    "    if gender: demographics.append(f\"{gender}\")\n",
    "    if age: demographics.append(f\"{age}-year-old\")\n",
    "    demographic_info = \" \".join(demographics)\n",
    "\n",
    "    structured_info = []\n",
    "    if diagnosis: structured_info.append(f\"Diagnosis: {diagnosis}\")\n",
    "    if medications: structured_info.append(f\"Medications: {medications}\")\n",
    "    if allergies: structured_info.append(f\"Allergies: {allergies}\")\n",
    "\n",
    "    # Final prompt construction\n",
    "    preamble = f\"Patient is a {demographic_info}.\"\n",
    "    if structured_info:\n",
    "        preamble += \" \" + \" \".join(structured_info)\n",
    "    prompt = f\"{preamble}\\nSummarize the following medical report:\\n{text}\"\n",
    "\n",
    "    # Token limit check\n",
    "    input_len = len(tokenizer.encode(prompt))\n",
    "    if input_len < MAX_INPUT_LENGTH:\n",
    "        return summarizer(prompt, max_length=SUMMARY_MAX, min_length=SUMMARY_MIN, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    # Chunking with context\n",
    "    summaries = []\n",
    "    for chunk in chunk_text(text, tokenizer):\n",
    "        chunk_prompt = f\"{preamble}\\n{chunk}\"\n",
    "        summary = summarizer(chunk_prompt, max_length=SUMMARY_MAX, min_length=SUMMARY_MIN, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Combine and summarize\n",
    "    final_summary = summarizer(\" \".join(summaries), max_length=SUMMARY_MAX*2, min_length=SUMMARY_MIN*2, do_sample=False)[0]['summary_text']\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k6X8Q69C5KJ",
    "outputId": "98517156-1095-4171-d4df-4f2998f9c97a"
   },
   "outputs": [],
   "source": [
    "# Original Medical Report vs Summary\n",
    "text = df_cleaned.iloc[0]['cleaned_transcription']\n",
    "summary = generate_medical_summary(text, age=\"23\", gender=\"female\")\n",
    "\n",
    "print(\"\\n Original Report:\\n\", text[:1000], \"...\\n\")  # Limit to 1000 chars for readability\n",
    "print(\" Generated Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ko7rImnuEIfL"
   },
   "outputs": [],
   "source": [
    "def generate_medical_summary(text, age=None, gender=None):\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # Better demographic prompt integration\n",
    "    if age and gender:\n",
    "        prompt = f\"Summarize the following medical report written for a {gender} aged {age}:\\n{text}\"\n",
    "    else:\n",
    "        prompt = f\"Summarize the following medical report:\\n{text}\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, truncation=False)\n",
    "    except Exception as e:\n",
    "        print(\"Encoding error:\", e)\n",
    "        return \"\"\n",
    "\n",
    "    if len(input_ids) < MAX_INPUT_LENGTH:\n",
    "        try:\n",
    "            return summarizer(prompt, max_length=min(SUMMARY_MAX, len(input_ids)), min_length=SUMMARY_MIN, do_sample=False)[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            print(\"Single pass error:\", e)\n",
    "            return \"\"\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in chunk_text(text, tokenizer):\n",
    "        try:\n",
    "            if age and gender:\n",
    "                chunk_prompt = f\"Summarize this report for a {gender} aged {age}:\\n{chunk}\"\n",
    "            else:\n",
    "                chunk_prompt = f\"Summarize this medical report:\\n{chunk}\"\n",
    "            summary = summarizer(chunk_prompt, max_length=SUMMARY_MAX, min_length=SUMMARY_MIN, do_sample=False)[0]['summary_text']\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(\"Chunk error:\", e)\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        final_prompt = \" \".join(summaries)\n",
    "        return summarizer(final_prompt, max_length=SUMMARY_MAX*2, min_length=SUMMARY_MIN*2, do_sample=False)[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        print(\"Final pass error:\", e)\n",
    "        return \" \".join(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8YlkUHT0FCVE",
    "outputId": "38a8e0f8-1c9f-4491-e339-d3660bb98815"
   },
   "outputs": [],
   "source": [
    "#  Comparison Table for Multiple Records\n",
    "# To evaluate or showcase the summarization pipeline across multiple samples.\n",
    "df_display = df_cleaned.head(30).copy()\n",
    "df_display['generated_summary'] = df_display['cleaned_transcription'].apply(\n",
    "    lambda x: generate_medical_summary(x, age=\"50\", gender=\"female\")\n",
    ")\n",
    "display(df_display[['description', 'cleaned_transcription', 'generated_summary']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuUVt2t-wErd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
